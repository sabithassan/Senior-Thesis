{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Senior Thesis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# October 14 report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Senior thesis presentation\n",
    "\n",
    "**Slides:**\n",
    "https://docs.google.com/presentation/d/1HyOsfYlXgrVh_gg-VyO0K1NHu7guCHN4-YfDTADX1J0/edit?usp=sharing\n",
    "\n",
    "**Anticipated questions:**\n",
    "\n",
    "1. How's active testing different from active learning?\n",
    "2. Active learning/testing done one by one or by batch?\n",
    "3. Computational time increased a lot?\n",
    "4. Retrain the whole model or modify the model?\n",
    "5. How's is it distinct?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Active Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Experiment 2: Active learning frameworks on multi-class classifiers\n",
    "\n",
    "**Data:**\n",
    "\n",
    "Supervectors constructed from recordings of 10 different speakers. There are 960 supervectors available from 960 recordings (96 from each speaker)\n",
    "\n",
    "**Experiment:** \n",
    "\n",
    "A passive learner and an active learner are trained from the supervectors. Tested different active learning frameworks against passive learning\n",
    "\n",
    "***Passive learner:***\n",
    "1. Sampled **N** number of supervectors for passive learner as training data. Trained a linear SVM in the setting described earlier.\n",
    "2. Tested with 40 recordings of class 1 speaker, 360 recordings of other speakers.\n",
    "\n",
    "***Active learner:***\n",
    "\n",
    "1. Sampled **INIT** number of supervectors for active learner as initial training data. Trained a linear SVM in the setting described earlier.\n",
    "2. **NOTHERS = N-INIT** number of supervectors are sampled one after another. After each sample, the model is modified.\n",
    "3. Frameworks tested based on different measures of uncertainty:\n",
    "   * Entropy based\n",
    "   * Least confidence based\n",
    "   * Margin based\n",
    "4. Tested with 40 recordings of class 1 speaker, 360 recordings of other speakers.\n",
    "\n",
    "**Results:**\n",
    "\n",
    "1. Y axis represents accuracy score\n",
    "2. Active learner in general performed better with same number of total samples\n",
    "3. As **N** increases, active learner and passive learner behave more similarly.\n",
    "4. If **N** drops too much, they start behaving similarly again.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![title](actVspasMult.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Couldn't conduct GMM based classification. My implementation took too long to run for active learning. Retraining GMMs after each sample seems computationally expensive. Batch based active learning?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formulation of active learning within SVMs\n",
    "\n",
    "$$ f:X \\rightarrow Z \\text { is the mapping by the model}$$\n",
    "$$ L:Z \\times X \\times Y \\rightarrow \\mathbb{R^+} \\text{ is the loss function}$$\n",
    "\n",
    "Goal is to minimize risk of f:\n",
    "\n",
    "$$ R(f) = \\mathbb{E_{(x,y)\\leadsto{D}}} \\lbrack L(f(x),x,y) \\rbrack $$ \n",
    "\n",
    "\n",
    "Soft margin SVM:\n",
    "\n",
    "$$ \\underset{w,b}{min} \\frac{1}{2} \\| {w^2}\\| \\sum_{i = 1}^l C_iL_{hinge}(\\langle \\textbf{w},\\phi(x_i)\\rangle + b, y_i) $$\n",
    "\n",
    "$$ L_{hinge}(f(x_i), y_i) = max(0, 1 − y_if(x_i)) \\text{ hinge loss function} $$\n",
    "\n",
    "\n",
    "*query the sample that ideally halves the version space. Therefore, we want to query the sample xˆ that induces a hyperplane as close to w as possible*\n",
    "\n",
    "*Simple Margin:*\n",
    "$$ \\hat{x} = \\underset{x \\in \\mathbb{U}}{argmin} \\| \\langle \\textbf{w},\\phi(x)\\rangle \\|$$\n",
    "\n",
    "*Max-Min margin:*\n",
    "Two SVMs, one positive, one negative.\n",
    "$$ m^+ = + \\langle \\textbf{w},\\phi(x)\\rangle, m^- = - \\langle \\textbf{w},\\phi(x)\\rangle $$\n",
    "$$ \\text{Query the one with max of } min(m^+,m^-) $$\n",
    "\n",
    "*Largest error:*\n",
    "$$ \\hat{x} = \\underset{x \\in \\mathbb{U}}{argmax} \\frac{1}{2}\\lbrack max(0,1-f(x)) + max(0,1+f(x))\\rbrack $$\n",
    "\n",
    "\n",
    "after some sampling:\n",
    "$$ \\hat{x} = \\underset{x \\in \\mathbb{U}}{argmax} {\\hspace{2mm} min} \\{\\lbrack max(0,1-f(x)) + max(0,1+f(x))\\rbrack \\}= \\underset{x \\in \\mathbb{U}}{argmin} \\|f(x)\\| $$\n",
    "\n",
    "\n",
    "The following need more understanding:\n",
    "\n",
    "1. Expected model change\n",
    "2. Combine Informativeness and Representative: \n",
    "3. Semi supervised active learning: uncertainty sampling and clustering\n",
    "4. Importance-Weighted Active Learning -> biased training data\n",
    "5. Multiclass active learning\n",
    "6. Online learning\n",
    "7. Batch-Mode active learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Active Testing\n",
    "\n",
    "\n",
    "\n",
    "** Lecture 1 **\n",
    "\n",
    "Slide 46: what is eta?\n",
    "\n",
    "** Lecture 2 **\n",
    "Slide 15: can't understand the structure yet\n",
    "\n",
    "Slide 56: K = N, still exponential?\n",
    "\n",
    "Slide 91: Why infinite? there is no boundary?\n",
    "Slide 92 -97: How is the optimal depth and number of neurons calculated?\n",
    "Slide 109/110: What are the red circles, which h<sub>i<\\sub> correspond to which red circle?\n",
    "Slide 121: What exactly is RBF network? How does it produce cylindrical outputs?\n",
    "\n",
    "** Lecture 3 **\n",
    "\n",
    "Slide 13: Structure of network given. Need to learn the weights of the arrows, biases.\n",
    "Slide 20: why is there noise?\n",
    "Slide 28: N+1th weight is b?\n",
    "Slide 32: normal vector?\n",
    "Slide 50, 52: How to determine correct label?\n",
    "Slide 57: differentiating with respect to each wi? what is eta? ****\n",
    "Slide 62: update on the overall error?\n",
    "Slide 66: z closest to zero among all the nodes?\n",
    "Slide 71: why a flat, non differentiatble function?\n",
    "Slide 73: what is the interesting interpretation?\n",
    "Slide 92: sigma(z) can be a sigmoid?\n",
    "Slide 132: what is f', What must step be to ensure we actually get to the optimum?\n",
    "Slide 135: I don't understand the function\n",
    "Slide 140: Do we need previous stuff?\n",
    "\n",
    "** Lecture 4 **\n",
    "Slide 62: Why ReLu left blank?\n",
    "Slide 63: Multiple coupled: bunch of outputs from bunch of neurons?\n",
    "Slide 74: Sigmoid activation?\n",
    "Slide 83: scaled L2 divergence. Why scaled?\n",
    "Slide 106: what's the derivative?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# October 7 report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Active Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notations\n",
    "\n",
    "$$ N = \\text{number of instances} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ X = \\{x_i \\hspace{4mm}|\\hspace{4mm} 1 \\le i \\le N\\} \\text{ (dataset)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Y = \\{{y_i} \\hspace{4mm} |\\hspace{4mm} 1 \\le i \\le N \\} \\text{(Set of true labels)} $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ D = X \\times Y $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ L = \\{({x_i}, {y_i})  \\hspace{4mm} | \\hspace{4mm}  {x_i} \\in X, {y_i} \\in Y \\} \\text { (labeled dataset, binary classes)}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ C({x_i}) \\text {= classifier output} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbb{H} = \\{{h : X -> Y} \\} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ err({h}) = Pr(h(X) != Y) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ {h^*} = argmin\\{err(h): h \\in \\mathbb{H}\\} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perfect boundary (Simplest case)\n",
    "\n",
    "If D is perfectly segregated into two classes by a boundary and data is univariate, with **logN** queries, we can find the classification boundary. Binary search by picking a median point.\n",
    "\n",
    "*Algorithms for Active Learning\n",
    "Daniel Joseph Hsu*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uncertainty sampling\n",
    "\n",
    "Query the instance x that learner is most uncertain about. Measure of uncertainty: \n",
    "\n",
    "**Entropy**\n",
    " \n",
    "$$ \\Phi{_{Entropy}} = - \\sum_{y}{P_\\theta}(y|x)log{P_\\theta}(y|x) $$\n",
    "\n",
    "We will query the instance with maximum entropy\n",
    "\n",
    "**smallest margin**\n",
    "\n",
    "$$ \\Phi{_M} = {P_\\theta}({y^*_1}|x) - {P_\\theta}({y^*_2}|x) \\text{ where }{y^*_1} \\text{ is the most likely label and } {y^*_2} \\text{ is the second most likely label for x} $$\n",
    "\n",
    "we query smallest instance that has smallest margin.\n",
    "\n",
    "**Least confident**\n",
    "\n",
    "$$ \\Phi{_M} = 1 - {P_\\theta}({y^*}|x)\\text{ where }{y^*} \\text{ is the most likely label} $$\n",
    "\n",
    "we pick the least confident instance\n",
    "\n",
    "*https://www.cs.cmu.edu/~tom/10701_sp11/recitations/Recitation_13.pdf*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PAC (Probably approximately correct learning) \n",
    "\n",
    "$$ \\text{assume } {h^*} \\text{ exists, } err({h^*}) = 0. \\text{Then, } err(h) = Pr(h(X) != {h^*}(X)) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Label complexity:** number of label queries needed so that algorithm produces a hypothesis $$ h \\in \\mathbb{H} \\text{ such that, } err(h) \\le err({h^*}) + \\epsilon \\text{ with probability } 1-\\delta $$\n",
    "\n",
    "for labeled subset, Z ⊂ X × Y, **version space V(Z)** := {h ∈ H : h(x) = y ∀(x, y) ∈ Z}\n",
    "\n",
    "For sample x<sub>t</sub>, if there is disagreement among V(Z<sub>t</sub>), the algorithm queries y<sub>t</sub>\n",
    "\n",
    "**Region for disagreement:**\n",
    "R(H') := {x ∈ X : ∃h, h′ ∈ H' such that h(x) != h(x)}\n",
    "\n",
    "If there is no disagreement for x<sub>t</sub>, V(Z<sub>t</sub>) = V(Z<sub>t-1</sub>) and y<sub>i</sub> is the label agreed on by all h in V(Z<sub>t</sub>)\n",
    "\n",
    "*Algorithms for Active Learning\n",
    "Daniel Joseph Hsu*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected Error reduction\n",
    "\n",
    "Aims to reduce overall error after an instance is queried.\n",
    "\n",
    "$$ R(x) = \\sum_{u \\in \\mathbb{X}} {E_y} \\lbrack {H_{\\theta} + (x,y)} (Y|u) \\rbrack $$\n",
    "\n",
    "\n",
    "*https://www.cs.cmu.edu/~tom/10701_sp11/recitations/Recitation_13.pdf*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noisy oracle\n",
    "\n",
    "### Human like noisy oracle\n",
    "\n",
    "** Assumption 1: **\n",
    "\n",
    "$$ \\text{let O(x) be the confidence of oracle on instance x. Let } \\sigma(x) \\text{ be the probability that oracle is wrong on x.} $$\n",
    "\n",
    "$$ \\sigma(x) = f(O(x)) \\text{ f is a monotonically decreasing function. if O(x)>O(x'), then } \\sigma(x) < \\sigma(x') $$\n",
    "\n",
    "* O(x) is not readily observable. learner trains to be target model, which should behave like the oracle, can compute posterior probabilities.\n",
    "\n",
    "** Assumption 2: **\n",
    "\n",
    "* O(x) is related to posterior probability by target model. \n",
    "\n",
    "$$ \\text {Let, } {y_{max}} = argmax_{y}p(y|x), \\text{ and }  p({y_{max}}|x) \\text{ is the maximum posterior probability by target model} $$\n",
    "\n",
    "$$ O(x) = g(p({y_{max}}|x)) $$\n",
    "\n",
    "$$ \\text{g is a monotonically increasing function. } g(p({y_{max}}|x))>g(p({y_{max}}|x')) => O(x)>O(x') $$\n",
    "\n",
    "$$ \\text {so, } g(p({y_{max}}|x))>g(p({y_{max}}|x')) => \\sigma(x) < \\sigma(x') $$\n",
    "\n",
    "$$ \\text{if g(x) = x and f(x) = 1-x then, } \\sigma(x) = min\\{p(1|x),p(0|x)\\} \\text{in case of binary classification} $$\n",
    "\n",
    "\n",
    "* f(x) can be changed to produce gaussian-like/ laplace like relations\n",
    "\n",
    "* conflict with uncertainty sampling: uncertain samples are more likely to be wrongly labeled by oracle. uncertain samples contain more information.\n",
    "\n",
    "*Active Learning with Human-Like Noisy Oracle, Jun Du, Charles X. Ling*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple noisy oracles\n",
    "\n",
    "$$ \\text {Let } Z = \\{{z_j^i} \\text { where } {z_j^i} \\text { is the label for ith instance by jth oracle} $$\n",
    "$$ P(Z|X) = \\sum_{Y} P(Z|X,Y)P(Y|X) $$\n",
    "\n",
    "\n",
    "** assumes ** P(Z|X,Y) = P(Z|Y). It means oracles' expertise sample independent.\n",
    "\n",
    "Y is a hidden variable here. We cannot observe Y.\n",
    "\n",
    "0<=P(Z=k|Y=k)<=1\n",
    "\n",
    "We need to find:\n",
    "\n",
    "$$ ({p_{Z|Y}^*}, {p_{Y|X}^*}) = {argmax_{{P_{Z|Y}},{P_{Y|X}}}} p(Z|X) $$\n",
    "\n",
    "The following part needs more understanding if this is to be explored.\n",
    "\n",
    "*A probabilistic model of active learning with multiple noisy oracles\n",
    "Weining Wu, Yang Liu, Maozu Guo n , Chunyu Wang, Xiaoyan Liu *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple noisy oracles with time varying expertise\n",
    "\n",
    "$$ \\text{ Let }\\phi(t)\\text{ denote oracle accuracy at time t. }$$\n",
    "$$ \\phi_{t} = {f_t}(\\phi_{t-1}, \\Delta_{t-1}) $$\n",
    "$$ = \\phi_{t-1} + \\Delta_{t-1} $$\n",
    "\n",
    "$$ \\Delta_t\\text{ is a zero mean gaussian with variance }\\sigma{^2} \\sigma{^2} \\text{ is upper bounded by some preknown value } $$\n",
    "**assumption** accuracy is between 0.5 and 1.\n",
    "$$ p(\\phi_{t}|\\phi_{t-1},\\sigma,0.5,1) = \\frac{\\frac{1}{\\sigma}\\beta(\\frac{\\phi_{t}-\\phi_{t-1}}{\\sigma})}{\\Phi(\\frac{1-\\phi_{t-1}}{\\sigma}) - \\Phi(\\frac{0.5-\\phi_{t-1}}{\\sigma})}$$\n",
    "\n",
    "$$ \\beta \\text{ and }\\Phi \\text{ are pdf and cdf of normal distribution with zero mean and variance } \\sigma{^2} $$\n",
    "\n",
    "$$ {z_t^j} \\text{ is the label predicted by oracle j at with expertise } \\phi_t $$\n",
    "$$ p({z_t^j}|\\phi{_t^j},y_t) = \\phi{_t^{j^{I(z{_t^j}=y_t)}}} (1-\\phi){_t^{j^{I(z{_t^j}=y_t)}}}$$\n",
    "\n",
    "y<sub>t</sub> is unknown. It's estimated from other labelers. Need better understanding to see what's happening.\n",
    "\n",
    "*A Probabilistic Framework to Learn from Multiple Annotators with Time-Varying Accuracy*\n",
    "*Pinar Donmez, Jaime Carbonell, Jeff Schneider *\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions \n",
    "\n",
    "* PAC is version space reduction?\n",
    "\n",
    "* Can we have synthesized queries?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1: Margin-based active learning framework\n",
    "\n",
    "**Data:**\n",
    "\n",
    "Supervectors constructed from recordings of 10 different speakers. There are 960 supervectors available from 960 recordings (96 from each speaker)\n",
    "\n",
    "**Experiment:** \n",
    "\n",
    "A passive learner and an active learner are trained from the supervectors. The training is done in **oneVall** setting for speaker verification task. The learners are trained on particular speaker as class 1, and every other speaker as class 0, resulting in binary classification.\n",
    "\n",
    "***Passive learner:***\n",
    "1. Sampled **N** number of supervectors for passive learner as training data. Trained a linear SVM in the setting described earlier.\n",
    "2. Tested with 40 recordings of class 1 speaker, 360 recordings of other speakers.\n",
    "\n",
    "***Active learner:***\n",
    "\n",
    "1. Sampled **INIT** number of supervectors for active learner as initial training data. Trained a linear SVM in the setting described earlier.\n",
    "2. **OTHERS = N-INIT** number of supervectors are sampled one after another. After each sample, the model is modified. The samples are selected based on margin based approach discussed earlier.\n",
    "3. Tested with 40 recordings of class 1 speaker, 360 recordings of other speakers.\n",
    "\n",
    "**Results:**\n",
    "\n",
    "1. **FRR** represents false rejection rate. **FAR** represents false acceptance rate\n",
    "2. Active learner performed better with same number of total samples\n",
    "3. As **N** increases, active learner and passive learner behave more similarly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](activeVpassive.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Active Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meeting with Anurag\n",
    "\n",
    "* Soundnet might not give good representation. Suggested not to implement soundnet but rather, use existing models.\n",
    "* Most formulations are restricted to binary classification. Extend formulation to multiple classes.\n",
    "* Extending to evaluation of multiple classifiers is a good idea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meeting with Professor Saquib\n",
    "\n",
    "* Get a clearer idea of why we are doing what we are doing. Why and how it's important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meetings with Shaden\n",
    "\n",
    "** Convolutional Neural Network **\n",
    "\n",
    "Convolution Neural Network: Basic understanding of structure of Neural Network, purpose of Backpropagation. Backpropagation needs to be understood more thoroughly. Training procedure not clear yet. Questions:\n",
    "1. How to transfer filters from video to audio?\n",
    "2. Aren't we losing information about periodicity? \n",
    "3. How are varying length dealt with during training?\n",
    "4. If audio vectors are represented in a 2D matrix, what are the filters? There is no dependency between the rows, right?\n",
    "\n",
    "** HMM clustering **\n",
    "Rather than one HMM, there should be multiple local HMMs. Arrows within local HMMs learned using standard learning procedure. Problems: \n",
    "1. how to segment audio into local HMMs, \n",
    "2. what are the arrows between local HMMs? Uniform? learned from data -> (needs a lot of data)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
