{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Senior Thesis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## October 29 report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 5: Margin-based active learning framework with weak labels\n",
    "\n",
    "**Data:**\n",
    "\n",
    "Supervectors constructed from recordings of 10 different speakers. There are 960 supervectors available from 960 recordings (96 from each speaker)\n",
    "\n",
    "**Experiment:** \n",
    "\n",
    "A passive learner, a normal active learner and an active learner with weak labels are trained from the supervectors. The training is done in **oneVall** setting for speaker verification task. The learners are trained on particular speaker as class 1, and every other speaker as class 0, resulting in binary classification.\n",
    "\n",
    "***Passive learner:***\n",
    "1. Sampled **N** number of supervectors for passive learner as training data. Trained a linear SVM in the setting described earlier.\n",
    "2. Tested with 40 recordings of class 1 speaker, 360 recordings of other speakers.\n",
    "\n",
    "***Active learner:***\n",
    "\n",
    "1. Sampled **INIT** number of supervectors for active learner as initial training data. Trained a linear SVM in the setting described earlier.\n",
    "2. **OTHERS = N-INIT** number of supervectors are sampled one after another. After each sample, the model is modified. The samples are selected based on margin based approach discussed earlier.\n",
    "3. generated weak labels by the following formula:\n",
    "    * p = random.uniform(0.75, 0.90) if sample is positive\n",
    "    * p = random.uniform(0.1, 0.25) if sample is negative\n",
    "    \n",
    "4. NINT are seleceted according to weak labels. Used kmeans with K = 5 to stratify the weak labels. Used equal allocation.    \n",
    "\n",
    "5. Tested with 40 recordings of class 1 speaker, 360 recordings of other speakers.\n",
    "\n",
    "\n",
    "**Results:**\n",
    "\n",
    "1. **FRR** represents false rejection rate. **FAR** represents false acceptance rate\n",
    "2. Active learner performed better with same number of total samples\n",
    "3. As **N** increases, active learner and passive learner behave more similarly.\n",
    "4. Active learner with \"weak labels\" generally perform better\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](weak.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 4::\n",
    "\n",
    "** Let the active learning SVM be S **\n",
    "1. Keep an SVM, M, trained with weak labels. \n",
    "2. When true labels are obtained, update M with the weak labels\n",
    "3. When selecting samples, consult both S and M\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### also classifier trained with weak labels + true labels \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![title](weak2.jpg)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3: Margin-based active learning framework with weak labels\n",
    "\n",
    "**Data:**\n",
    "\n",
    "Supervectors constructed from recordings of 10 different speakers. There are 960 supervectors available from 960 recordings (96 from each speaker)\n",
    "\n",
    "**Experiment:** \n",
    "\n",
    "A passive learner, and an active learner with maxmin method are trained from the supervectors. The training is done in **oneVall** setting for speaker verification task. The learners are trained on particular speaker as class 1, and every other speaker as class 0, resulting in binary classification.\n",
    "\n",
    "***Passive learner:***\n",
    "1. Sampled **N** number of supervectors for passive learner as training data. Trained a linear SVM in the setting described earlier.\n",
    "2. Tested with 40 recordings of class 1 speaker, 360 recordings of other speakers.\n",
    "\n",
    "***Active learner:***\n",
    "\n",
    "1. Sampled **INIT** number of supervectors for active learner as initial training data. Trained a linear SVM in the setting described earlier.\n",
    "2. **OTHERS = N-INIT** number of supervectors are sampled one after another. After each sample, the model is modified. The samples are selected based on margin based approach discussed earlier.\n",
    "\n",
    "3. for each sample, train SVMs with label 0 and label 1. m+, m- denote the distances respectively of this sample. \n",
    "4. Sample the instance with max(min(m+,m-))\n",
    "    \n",
    "5. Tested with 40 recordings of class 1 speaker, 360 recordings of other speakers.\n",
    "\n",
    "\n",
    "**Results:**\n",
    "\n",
    "1. **FRR** represents false rejection rate. **FAR** represents false acceptance rate\n",
    "2. Active learner performed better with same number of total samples\n",
    "3. As **N** increases, active learner and passive learner behave more similarly.\n",
    "\n",
    "![title](maxmin.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Neural network trying out\n",
    "\n",
    "Used Keras.\n",
    "\n",
    "optimizer: RMSProp, no learning rate mentioned\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "batch size = 128\n",
    "\n",
    "Accuracy 98.19%\n",
    "\n",
    "***************************************\n",
    "optimizer: RMSProp, no learning rate mentioned\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "batch size = 2000\n",
    "\n",
    "dropout = 0.1 after each activation layer\n",
    "\n",
    "batch normalization after each hidden layer\n",
    "\n",
    "Accuracy 94.67%\n",
    "\n",
    "********************************************\n",
    "optimizer: RMSProp, learning rate = 0.001\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "batch size = 2000\n",
    "\n",
    "dropout = 0.1 after each activation layer\n",
    "\n",
    "batch normalization after each hidden layer\n",
    "\n",
    "Accuracy 94.68%\n",
    "\n",
    "********************************************\n",
    "optimizer: RMSProp, learning rate = 0.01\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "batch size = 2000\n",
    "\n",
    "dropout = 0.1 after each activation layer\n",
    "\n",
    "batch normalization after each hidden layer\n",
    "\n",
    "Accuracy 97.68%\n",
    "\n",
    "********************************************\n",
    "optimizer: RMSProp, learning rate = 0.01\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "batch size = 2000\n",
    "\n",
    "dropout = 0.01 after each activation layer\n",
    "\n",
    "batch normalization after each hidden layer\n",
    "\n",
    "Accuracy 95.26%\n",
    "\n",
    "********************************************\n",
    "optimizer: RMSProp, learning rate = 0.01\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "batch size = 2000\n",
    "\n",
    "dropout = 0.25 after each activation layer\n",
    "\n",
    "batch normalization after each hidden layer\n",
    "\n",
    "Accuracy 97.85%\n",
    "\n",
    "\n",
    "\n",
    "## Active Learning -> S. Tong. Active learning: Theory and applications, phd thesis\n",
    "\n",
    "p.19 -> why is it a convex problem?\n",
    "        norm of phi(x) fixed?\n",
    "        \n",
    "        \n",
    "p.39 -> what is parameter space W?       \n",
    "p/42 -> reduce version space size\n",
    "p.46 -> isn't maxmin extremely expensive?\n",
    "\n",
    "## Neural networks\n",
    "\n",
    "** Lecture 4 **\n",
    "\n",
    "Backpropagation\n",
    "\n",
    "** Lecture 5 **\n",
    "\n",
    "1. backprogation doesn't find optimal solution\n",
    "2. perceptron may swing dramatically on new small input\n",
    "3. backprop doesn't change dramatically on new small input\n",
    "4. slide 33: overdesign?\n",
    "5. slide 34: saddle point. backprop will not slide on saddle point and reach local minimum? \n",
    "6. slide 41: only until gradient is 0? It may diverge?\n",
    "7. Slide 42: why exponentially fast?\n",
    "8. slide 44: why is quadratic optimization important\n",
    "9. slide 45: how does convergence, divergence and oscillating convergence work in terms of optimal step size?\n",
    "10. slide 48: equal value contour parallel to the axis?\n",
    "11. slide 51: uncoupled descents\n",
    "12. slide 54: which one is what?\n",
    "13. slide 59: scale the axis\n",
    "14. slide 74: whats happening here\n",
    "15. slide 88: decaying learning rate. assume hessian normalization? escapes local minima\n",
    "16. slide 89: what is alpha*eta*\n",
    "17. slide 92: Many of the convergence issues arise because we force the same learning rate on all parameters\n",
    "18. slide 108: quickprop?\n",
    "** WHAT IS THE independent PARAMETERs **\n",
    "* slide 115: momentum based method\n",
    "* slide 126: nestorov's accelerated gradient\n",
    "\n",
    "\n",
    "** Lecture 6 **\n",
    "* Stochastic gradient descent\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# October 14 report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Senior thesis presentation\n",
    "\n",
    "**Slides:**\n",
    "https://docs.google.com/presentation/d/1HyOsfYlXgrVh_gg-VyO0K1NHu7guCHN4-YfDTADX1J0/edit?usp=sharing\n",
    "\n",
    "**Anticipated questions:**\n",
    "\n",
    "1. How's active testing different from active learning?\n",
    "2. Active learning/testing done one by one or by batch?\n",
    "3. Computational time increased a lot?\n",
    "4. Retrain the whole model or modify the model?\n",
    "5. How's is it distinct?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Active Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Experiment 2: Active learning frameworks on multi-class classifiers\n",
    "\n",
    "**Data:**\n",
    "\n",
    "Supervectors constructed from recordings of 10 different speakers. There are 960 supervectors available from 960 recordings (96 from each speaker)\n",
    "\n",
    "**Experiment:** \n",
    "\n",
    "A passive learner and an active learner are trained from the supervectors. Tested different active learning frameworks against passive learning\n",
    "\n",
    "***Passive learner:***\n",
    "1. Sampled **N** number of supervectors for passive learner as training data. Trained a linear SVM in the setting described earlier.\n",
    "2. Tested with 40 recordings of class 1 speaker, 360 recordings of other speakers.\n",
    "\n",
    "***Active learner:***\n",
    "\n",
    "1. Sampled **INIT** number of supervectors for active learner as initial training data. Trained a linear SVM in the setting described earlier.\n",
    "2. **NOTHERS = N-INIT** number of supervectors are sampled one after another. After each sample, the model is modified.\n",
    "3. Frameworks tested based on different measures of uncertainty:\n",
    "   * Entropy based\n",
    "   * Least confidence based\n",
    "   * Margin based\n",
    "4. Tested with 40 recordings of class 1 speaker, 360 recordings of other speakers.\n",
    "\n",
    "**Results:**\n",
    "\n",
    "1. Y axis represents accuracy score\n",
    "2. Active learner in general performed better with same number of total samples\n",
    "3. As **N** increases, active learner and passive learner behave more similarly.\n",
    "4. If **N** drops too much, they start behaving similarly again.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![title](actVspasMult.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Couldn't conduct GMM based classification. My implementation took too long to run for active learning. Retraining GMMs after each sample seems computationally expensive. Batch based active learning?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formulation of active learning within SVMs\n",
    "\n",
    "$$ f:X \\rightarrow Z \\text { is the mapping by the model}$$\n",
    "$$ L:Z \\times X \\times Y \\rightarrow \\mathbb{R^+} \\text{ is the loss function}$$\n",
    "\n",
    "Goal is to minimize risk of f:\n",
    "\n",
    "$$ R(f) = \\mathbb{E_{(x,y)\\leadsto{D}}} \\lbrack L(f(x),x,y) \\rbrack $$ \n",
    "\n",
    "\n",
    "Soft margin SVM:\n",
    "\n",
    "$$ \\underset{w,b}{min} \\frac{1}{2} \\| {w^2}\\| \\sum_{i = 1}^l C_iL_{hinge}(\\langle \\textbf{w},\\phi(x_i)\\rangle + b, y_i) $$\n",
    "\n",
    "$$ L_{hinge}(f(x_i), y_i) = max(0, 1 − y_if(x_i)) \\text{ hinge loss function} $$\n",
    "\n",
    "\n",
    "*query the sample that ideally halves the version space. Therefore, we want to query the sample xˆ that induces a hyperplane as close to w as possible*\n",
    "\n",
    "*Simple Margin:*\n",
    "$$ \\hat{x} = \\underset{x \\in \\mathbb{U}}{argmin} \\| \\langle \\textbf{w},\\phi(x)\\rangle \\|$$\n",
    "\n",
    "*Max-Min margin:*\n",
    "Two SVMs, one positive, one negative.\n",
    "$$ m^+ = + \\langle \\textbf{w},\\phi(x)\\rangle, m^- = - \\langle \\textbf{w},\\phi(x)\\rangle $$\n",
    "$$ \\text{Query the one with max of } min(m^+,m^-) $$\n",
    "\n",
    "*Largest error:*\n",
    "$$ \\hat{x} = \\underset{x \\in \\mathbb{U}}{argmax} \\frac{1}{2}\\lbrack max(0,1-f(x)) + max(0,1+f(x))\\rbrack $$\n",
    "\n",
    "\n",
    "after some sampling:\n",
    "$$ \\hat{x} = \\underset{x \\in \\mathbb{U}}{argmax} {\\hspace{2mm} min} \\{\\lbrack max(0,1-f(x)) + max(0,1+f(x))\\rbrack \\}= \\underset{x \\in \\mathbb{U}}{argmin} \\|f(x)\\| $$\n",
    "\n",
    "\n",
    "The following need more understanding:\n",
    "\n",
    "1. Expected model change\n",
    "2. Combine Informativeness and Representative: \n",
    "3. Semi supervised active learning: uncertainty sampling and clustering\n",
    "4. Importance-Weighted Active Learning -> biased training data\n",
    "5. Multiclass active learning\n",
    "6. Online learning\n",
    "7. Batch-Mode active learning\n",
    "\n",
    "*Active Learning with Support Vector Machines, Jan Kremer, Kim Steenstrup Pedersen, Christian Igel*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Active Testing\n",
    "\n",
    "\n",
    "\n",
    "** Lecture 1 **\n",
    "\n",
    "Slide 46: what is eta?\n",
    "\n",
    "** Lecture 2 **\n",
    "* Slide 15: can't understand the structure yet\n",
    "\n",
    "* Slide 56: K = N, still exponential?\n",
    "\n",
    "* Slide 91: Why infinite? there is no boundary?\n",
    "* Slide 92 -97: How is the optimal depth and number of neurons calculated?\n",
    "* Slide 109/110: What are the red circles, which h<sub>i<\\sub> correspond to which red circle?\n",
    "* Slide 121: What exactly is RBF network? How does it produce cylindrical outputs?\n",
    "\n",
    "** Lecture 3 **\n",
    "\n",
    "* Slide 13: Structure of network given. Need to learn the weights of the arrows, biases.\n",
    "* Slide 20: why is there noise?\n",
    "* Slide 28: N+1th weight is b?\n",
    "* Slide 32: normal vector?\n",
    "* Slide 50, 52: How to determine correct label?\n",
    "* Slide 57: differentiating with respect to each wi? what is eta? ****\n",
    "* Slide 62: update on the overall error?\n",
    "* Slide 66: z closest to zero among all the nodes?\n",
    "* Slide 71: why a flat, non differentiatble function?\n",
    "* Slide 73: what is the interesting interpretation?\n",
    "* Slide 92: sigma(z) can be a sigmoid?\n",
    "* Slide 132: what is f', What must step be to ensure we actually get to the optimum?\n",
    "* Slide 135: I don't understand the function\n",
    "* Slide 140: Do we need previous stuff?\n",
    "\n",
    "** Lecture 4 **\n",
    "* Slide 62: Why ReLu left blank?\n",
    "* Slide 63: Multiple coupled: bunch of outputs from bunch of neurons?\n",
    "* Slide 74: Sigmoid activation?\n",
    "* Slide 83: scaled L2 divergence. Why scaled?\n",
    "* Slide 106: what's the derivative?\n",
    "\n",
    "** Lecture 7 **\n",
    "* Slide 7:  the perceptron fires if the input vector is close enough to the weight vector\n",
    "* Slide 28: Shift invariance. Scan.\n",
    "* Slide 52: why a lot? Shared parameter.\n",
    "\n",
    "* compare slide 167 and slide 49: if they are identical, there is no need for multiple right?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# October 7 report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Active Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notations\n",
    "\n",
    "$$ N = \\text{number of instances} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ X = \\{x_i \\hspace{4mm}|\\hspace{4mm} 1 \\le i \\le N\\} \\text{ (dataset)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Y = \\{{y_i} \\hspace{4mm} |\\hspace{4mm} 1 \\le i \\le N \\} \\text{(Set of true labels)} $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ D = X \\times Y $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ L = \\{({x_i}, {y_i})  \\hspace{4mm} | \\hspace{4mm}  {x_i} \\in X, {y_i} \\in Y \\} \\text { (labeled dataset, binary classes)}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ C({x_i}) \\text {= classifier output} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbb{H} = \\{{h : X -> Y} \\} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ err({h}) = Pr(h(X) != Y) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ {h^*} = argmin\\{err(h): h \\in \\mathbb{H}\\} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perfect boundary (Simplest case)\n",
    "\n",
    "If D is perfectly segregated into two classes by a boundary and data is univariate, with **logN** queries, we can find the classification boundary. Binary search by picking a median point.\n",
    "\n",
    "*Algorithms for Active Learning\n",
    "Daniel Joseph Hsu*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uncertainty sampling\n",
    "\n",
    "Query the instance x that learner is most uncertain about. Measure of uncertainty: \n",
    "\n",
    "**Entropy**\n",
    " \n",
    "$$ \\Phi{_{Entropy}} = - \\sum_{y}{P_\\theta}(y|x)log{P_\\theta}(y|x) $$\n",
    "\n",
    "We will query the instance with maximum entropy\n",
    "\n",
    "**smallest margin**\n",
    "\n",
    "$$ \\Phi{_M} = {P_\\theta}({y^*_1}|x) - {P_\\theta}({y^*_2}|x) \\text{ where }{y^*_1} \\text{ is the most likely label and } {y^*_2} \\text{ is the second most likely label for x} $$\n",
    "\n",
    "we query smallest instance that has smallest margin.\n",
    "\n",
    "**Least confident**\n",
    "\n",
    "$$ \\Phi{_M} = 1 - {P_\\theta}({y^*}|x)\\text{ where }{y^*} \\text{ is the most likely label} $$\n",
    "\n",
    "we pick the least confident instance\n",
    "\n",
    "*https://www.cs.cmu.edu/~tom/10701_sp11/recitations/Recitation_13.pdf*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PAC (Probably approximately correct learning) \n",
    "\n",
    "$$ \\text{assume } {h^*} \\text{ exists, } err({h^*}) = 0. \\text{Then, } err(h) = Pr(h(X) != {h^*}(X)) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Label complexity:** number of label queries needed so that algorithm produces a hypothesis $$ h \\in \\mathbb{H} \\text{ such that, } err(h) \\le err({h^*}) + \\epsilon \\text{ with probability } 1-\\delta $$\n",
    "\n",
    "for labeled subset, Z ⊂ X × Y, **version space V(Z)** := {h ∈ H : h(x) = y ∀(x, y) ∈ Z}\n",
    "\n",
    "For sample x<sub>t</sub>, if there is disagreement among V(Z<sub>t</sub>), the algorithm queries y<sub>t</sub>\n",
    "\n",
    "**Region for disagreement:**\n",
    "R(H') := {x ∈ X : ∃h, h′ ∈ H' such that h(x) != h(x)}\n",
    "\n",
    "If there is no disagreement for x<sub>t</sub>, V(Z<sub>t</sub>) = V(Z<sub>t-1</sub>) and y<sub>i</sub> is the label agreed on by all h in V(Z<sub>t</sub>)\n",
    "\n",
    "*Algorithms for Active Learning\n",
    "Daniel Joseph Hsu*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected Error reduction\n",
    "\n",
    "Aims to reduce overall error after an instance is queried.\n",
    "\n",
    "$$ R(x) = \\sum_{u \\in \\mathbb{X}} {E_y} \\lbrack {H_{\\theta} + (x,y)} (Y|u) \\rbrack $$\n",
    "\n",
    "\n",
    "*https://www.cs.cmu.edu/~tom/10701_sp11/recitations/Recitation_13.pdf*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noisy oracle\n",
    "\n",
    "### Human like noisy oracle\n",
    "\n",
    "** Assumption 1: **\n",
    "\n",
    "$$ \\text{let O(x) be the confidence of oracle on instance x. Let } \\sigma(x) \\text{ be the probability that oracle is wrong on x.} $$\n",
    "\n",
    "$$ \\sigma(x) = f(O(x)) \\text{ f is a monotonically decreasing function. if O(x)>O(x'), then } \\sigma(x) < \\sigma(x') $$\n",
    "\n",
    "* O(x) is not readily observable. learner trains to be target model, which should behave like the oracle, can compute posterior probabilities.\n",
    "\n",
    "** Assumption 2: **\n",
    "\n",
    "* O(x) is related to posterior probability by target model. \n",
    "\n",
    "$$ \\text {Let, } {y_{max}} = argmax_{y}p(y|x), \\text{ and }  p({y_{max}}|x) \\text{ is the maximum posterior probability by target model} $$\n",
    "\n",
    "$$ O(x) = g(p({y_{max}}|x)) $$\n",
    "\n",
    "$$ \\text{g is a monotonically increasing function. } g(p({y_{max}}|x))>g(p({y_{max}}|x')) => O(x)>O(x') $$\n",
    "\n",
    "$$ \\text {so, } g(p({y_{max}}|x))>g(p({y_{max}}|x')) => \\sigma(x) < \\sigma(x') $$\n",
    "\n",
    "$$ \\text{if g(x) = x and f(x) = 1-x then, } \\sigma(x) = min\\{p(1|x),p(0|x)\\} \\text{in case of binary classification} $$\n",
    "\n",
    "\n",
    "* f(x) can be changed to produce gaussian-like/ laplace like relations\n",
    "\n",
    "* conflict with uncertainty sampling: uncertain samples are more likely to be wrongly labeled by oracle. uncertain samples contain more information.\n",
    "\n",
    "*Active Learning with Human-Like Noisy Oracle, Jun Du, Charles X. Ling*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple noisy oracles\n",
    "\n",
    "$$ \\text {Let } Z = \\{{z_j^i} \\text { where } {z_j^i} \\text { is the label for ith instance by jth oracle} $$\n",
    "$$ P(Z|X) = \\sum_{Y} P(Z|X,Y)P(Y|X) $$\n",
    "\n",
    "\n",
    "** assumes ** P(Z|X,Y) = P(Z|Y). It means oracles' expertise sample independent.\n",
    "\n",
    "Y is a hidden variable here. We cannot observe Y.\n",
    "\n",
    "0<=P(Z=k|Y=k)<=1\n",
    "\n",
    "We need to find:\n",
    "\n",
    "$$ ({p_{Z|Y}^*}, {p_{Y|X}^*}) = {argmax_{{P_{Z|Y}},{P_{Y|X}}}} p(Z|X) $$\n",
    "\n",
    "The following part needs more understanding if this is to be explored.\n",
    "\n",
    "*A probabilistic model of active learning with multiple noisy oracles\n",
    "Weining Wu, Yang Liu, Maozu Guo n , Chunyu Wang, Xiaoyan Liu *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple noisy oracles with time varying expertise\n",
    "\n",
    "$$ \\text{ Let }\\phi(t)\\text{ denote oracle accuracy at time t. }$$\n",
    "$$ \\phi_{t} = {f_t}(\\phi_{t-1}, \\Delta_{t-1}) $$\n",
    "$$ = \\phi_{t-1} + \\Delta_{t-1} $$\n",
    "\n",
    "$$ \\Delta_t\\text{ is a zero mean gaussian with variance }\\sigma{^2} \\sigma{^2} \\text{ is upper bounded by some preknown value } $$\n",
    "**assumption** accuracy is between 0.5 and 1.\n",
    "$$ p(\\phi_{t}|\\phi_{t-1},\\sigma,0.5,1) = \\frac{\\frac{1}{\\sigma}\\beta(\\frac{\\phi_{t}-\\phi_{t-1}}{\\sigma})}{\\Phi(\\frac{1-\\phi_{t-1}}{\\sigma}) - \\Phi(\\frac{0.5-\\phi_{t-1}}{\\sigma})}$$\n",
    "\n",
    "$$ \\beta \\text{ and }\\Phi \\text{ are pdf and cdf of normal distribution with zero mean and variance } \\sigma{^2} $$\n",
    "\n",
    "$$ {z_t^j} \\text{ is the label predicted by oracle j at with expertise } \\phi_t $$\n",
    "$$ p({z_t^j}|\\phi{_t^j},y_t) = \\phi{_t^{j^{I(z{_t^j}=y_t)}}} (1-\\phi){_t^{j^{I(z{_t^j}=y_t)}}}$$\n",
    "\n",
    "y<sub>t</sub> is unknown. It's estimated from other labelers. Need better understanding to see what's happening.\n",
    "\n",
    "*A Probabilistic Framework to Learn from Multiple Annotators with Time-Varying Accuracy*\n",
    "*Pinar Donmez, Jaime Carbonell, Jeff Schneider *\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions \n",
    "\n",
    "* PAC is version space reduction?\n",
    "\n",
    "* Can we have synthesized queries?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1: Margin-based active learning framework\n",
    "\n",
    "**Data:**\n",
    "\n",
    "Supervectors constructed from recordings of 10 different speakers. There are 960 supervectors available from 960 recordings (96 from each speaker)\n",
    "\n",
    "**Experiment:** \n",
    "\n",
    "A passive learner and an active learner are trained from the supervectors. The training is done in **oneVall** setting for speaker verification task. The learners are trained on particular speaker as class 1, and every other speaker as class 0, resulting in binary classification.\n",
    "\n",
    "***Passive learner:***\n",
    "1. Sampled **N** number of supervectors for passive learner as training data. Trained a linear SVM in the setting described earlier.\n",
    "2. Tested with 40 recordings of class 1 speaker, 360 recordings of other speakers.\n",
    "\n",
    "***Active learner:***\n",
    "\n",
    "1. Sampled **INIT** number of supervectors for active learner as initial training data. Trained a linear SVM in the setting described earlier.\n",
    "2. **OTHERS = N-INIT** number of supervectors are sampled one after another. After each sample, the model is modified. The samples are selected based on margin based approach discussed earlier.\n",
    "3. Tested with 40 recordings of class 1 speaker, 360 recordings of other speakers.\n",
    "\n",
    "**Results:**\n",
    "\n",
    "1. **FRR** represents false rejection rate. **FAR** represents false acceptance rate\n",
    "2. Active learner performed better with same number of total samples\n",
    "3. As **N** increases, active learner and passive learner behave more similarly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](activeVpassive.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Active Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meeting with Anurag\n",
    "\n",
    "* Soundnet might not give good representation. Suggested not to implement soundnet but rather, use existing models.\n",
    "* Most formulations are restricted to binary classification. Extend formulation to multiple classes.\n",
    "* Extending to evaluation of multiple classifiers is a good idea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meeting with Professor Saquib\n",
    "\n",
    "* Get a clearer idea of why we are doing what we are doing. Why and how it's important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meetings with Shaden\n",
    "\n",
    "** Convolutional Neural Network **\n",
    "\n",
    "Convolution Neural Network: Basic understanding of structure of Neural Network, purpose of Backpropagation. Backpropagation needs to be understood more thoroughly. Training procedure not clear yet. Questions:\n",
    "1. How to transfer filters from video to audio?\n",
    "2. Aren't we losing information about periodicity? \n",
    "3. How are varying length dealt with during training?\n",
    "4. If audio vectors are represented in a 2D matrix, what are the filters? There is no dependency between the rows, right?\n",
    "\n",
    "** HMM clustering **\n",
    "Rather than one HMM, there should be multiple local HMMs. Arrows within local HMMs learned using standard learning procedure. Problems: \n",
    "1. how to segment audio into local HMMs, \n",
    "2. what are the arrows between local HMMs? Uniform? learned from data -> (needs a lot of data)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
