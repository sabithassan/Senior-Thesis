{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Senior Thesis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# October 7 report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Active Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notations\n",
    "\n",
    "$$ N = \\text{number of instances} $$\n",
    "\n",
    "$$ X = \\{x_i \\hspace{4mm}|\\hspace{4mm} 1 \\le i \\le N\\} \\text{ (dataset)} $$\n",
    "\n",
    "$$ Y = \\{{y_i} \\hspace{4mm} |\\hspace{4mm} 1 \\le i \\le N \\} \\text{(Set of true labels)} $$ \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ D = X \\times Y $$\n",
    "\n",
    "$$ L = \\{({x_i}, {y_i})  \\hspace{4mm} | \\hspace{4mm}  {x_i} \\in X, {y_i} \\in Y \\} \\text { (labeled dataset, binary classes)}$$ \n",
    "\n",
    "$$ C({x_i}) \\text {= classifier output} $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbb{H} = \\{{h : X -> Y} \\} $$\n",
    "\n",
    "$$ err({h}) = Pr(h(X) != Y) $$\n",
    "\n",
    "$$ {h^*} = argmin\\{err(h): h \\in \\mathbb{H}\\} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perfect boundary (Simplest case)\n",
    "\n",
    "If D is perfectly segregated into two classes by a boundary and data is univariate, with **logN** queries, we can find the classification boundary. Binary search by picking a median point.\n",
    "\n",
    "*Algorithms for Active Learning\n",
    "Daniel Joseph Hsu*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uncertainty sampling\n",
    "\n",
    "Query the instance x that learner is most uncertain about. Measure of uncertainty: \n",
    "\n",
    "**Entropy**\n",
    " \n",
    "$$ \\Phi{_{Entropy}} = - \\sum_{y}{P_\\theta}(y|x)log{P_\\theta}(y|x) $$\n",
    "\n",
    "We will query the instance with maximum entropy\n",
    "\n",
    "**smallest margin**\n",
    "\n",
    "$$ \\Phi{_M} = {P_\\theta}({y^*_1}|x) - {P_\\theta}({y^*_2}|x) \\text{ where }{y^*_1} \\text{ is the most likely label and } {y^*_2} \\text{ is the second most likely label for x} $$\n",
    "\n",
    "we query smallest instance that has smallest margin.\n",
    "\n",
    "**Least confident**\n",
    "\n",
    "$$ \\Phi{_M} = 1 - {P_\\theta}({y^*}|x)\\text{ where }{y^*} \\text{ is the most likely label} $$\n",
    "\n",
    "we pick the least confident instance\n",
    "\n",
    "*https://www.cs.cmu.edu/~tom/10701_sp11/recitations/Recitation_13.pdf*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PAC (Probably approximately correct learning) \n",
    "\n",
    "$$ \\text{assume } {h^*} \\text{ exists, } err({h^*}) = 0. \\text{Then, } err(h) = Pr(h(X) != {h^*}(X)) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Label complexity:** number of label queries needed so that algorithm produces a hypothesis $$ h \\in \\mathbb{H} \\text{ such that, } err(h) \\le err({h^*}) + \\epsilon \\text{ with probability } 1-\\delta $$\n",
    "\n",
    "for labeled subset, Z ⊂ X × Y, **version space V(Z)** := {h ∈ H : h(x) = y ∀(x, y) ∈ Z}\n",
    "\n",
    "For sample x<sub>t</sub>, if there is disagreement among V(Z<sub>t</sub>), the algorithm queries y<sub>t</sub>\n",
    "\n",
    "**Region for disagreement:**\n",
    "R(H') := {x ∈ X : ∃h, h′ ∈ H' such that h(x) != h(x)}\n",
    "\n",
    "If there is no disagreement for x<sub>t</sub>, V(Z<sub>t</sub>) = V(Z<sub>t-1</sub>) and y<sub>i</sub> is the label agreed on by all h in V(Z<sub>t</sub>)\n",
    "\n",
    "*Algorithms for Active Learning\n",
    "Daniel Joseph Hsu*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected Error reduction\n",
    "\n",
    "Aims to reduce overall error after an instance is queried.\n",
    "\n",
    "$$ R(x) = \\sum_{u \\in \\mathbb{X}} {E_y} \\lbrack {H_{\\theta} + (x,y)} (Y|u) \\rbrack $$\n",
    "\n",
    "\n",
    "*https://www.cs.cmu.edu/~tom/10701_sp11/recitations/Recitation_13.pdf*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noisy oracle\n",
    "\n",
    "### Human like noisy oracle\n",
    "\n",
    "** Assumption 1: **\n",
    "\n",
    "$$ \\text{let O(x) be the confidence of oracle on instance x. Let } \\sigma(x) \\text{ be the probability that oracle is wrong on x.} $$\n",
    "\n",
    "$$ \\sigma(x) = f(O(x)) \\text{ f is a monotonically decreasing function. if O(x)>O(x'), then } \\sigma(x) < \\sigma(x') $$\n",
    "\n",
    "* O(x) is not readily observable. learner trains to be target model, which should behave like the oracle, can compute posterior probabilities.\n",
    "\n",
    "** Assumption 2: **\n",
    "\n",
    "* O(x) is related to posterior probability by target model. \n",
    "\n",
    "$$ \\text {Let, } {y_{max}} = argmax_{y}p(y|x), \\text{ and }  p({y_{max}}|x) \\text{ is the maximum posterior probability by target model} $$\n",
    "\n",
    "$$ O(x) = g(p({y_{max}}|x)) $$\n",
    "\n",
    "$$ \\text{g is a monotonically increasing function. } g(p({y_{max}}|x))>g(p({y_{max}}|x')) => O(x)>O(x') $$\n",
    "\n",
    "$$ \\text {so, } g(p({y_{max}}|x))>g(p({y_{max}}|x')) => \\sigma(x) < \\sigma(x') $$\n",
    "\n",
    "$$ \\text{if g(x) = x and f(x) = 1-x then, } \\sigma(x) = min\\{p(1|x),p(0|x)\\} \\text{in case of binary classification} $$\n",
    "\n",
    "\n",
    "* f(x) can be changed to produce gaussian-like/ laplace like relations\n",
    "\n",
    "* conflict with uncertainty sampling: uncertain samples are more likely to be wrongly labeled by oracle. uncertain samples contain more information.\n",
    "\n",
    "*Active Learning with Human-Like Noisy Oracle, Jun Du, Charles X. Ling*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple noisy oracles\n",
    "\n",
    "$$ \\text {Let } Z = \\{{z_j^i} \\text { where } {z_j^i} \\text { is the label for ith instance by jth oracle} $$\n",
    "$$ P(Z|X) = \\sum_{Y} P(Z|X,Y)P(Y|X) $$\n",
    "\n",
    "\n",
    "** assumes ** P(Z|X,Y) = P(Z|Y). It means oracles' expertise sample independent.\n",
    "\n",
    "Y is a hidden variable here. We cannot observe Y.\n",
    "\n",
    "0<=P(Z=k|Y=k)<=1\n",
    "\n",
    "We need to find:\n",
    "\n",
    "$$ ({p_{Z|Y}^*}, {p_{Y|X}^*}) = {argmax_{{P_{Z|Y}},{P_{Y|X}}}} p(Z|X) $$\n",
    "\n",
    "The following part needs more understanding if this is to be explored.\n",
    "\n",
    "*A probabilistic model of active learning with multiple noisy oracles\n",
    "Weining Wu, Yang Liu, Maozu Guo n , Chunyu Wang, Xiaoyan Liu *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple noisy oracles with time varying expertise\n",
    "\n",
    "$$ \\text{ Let }\\phi(t)\\text{ denote oracle accuracy at time t. }$$\n",
    "$$ \\phi_{t} = {f_t}(\\phi_{t-1}, \\Delta_{t-1}) $$\n",
    "$$ = \\phi_{t-1} + \\Delta_{t-1} $$\n",
    "\n",
    "$$ \\Delta_t\\text{ is a zero mean gaussian with variance }\\sigma{^2} \\sigma{^2} \\text{ is upper bounded by some preknown value } $$\n",
    "**assumption** accuracy is between 0.5 and 1.\n",
    "$$ p(\\phi_{t}|\\phi_{t-1},\\sigma,0.5,1) = \\frac{\\frac{1}{\\sigma}\\beta(\\frac{\\phi_{t}-\\phi_{t-1}}{\\sigma})}{\\Phi(\\frac{1-\\phi_{t-1}}{\\sigma}) - \\Phi(\\frac{0.5-\\phi_{t-1}}{\\sigma})}$$\n",
    "\n",
    "$$ \\beta \\text{ and }\\Phi \\text{ are pdf and cdf of normal distribution with zero mean and variance } \\sigma{^2} $$\n",
    "\n",
    "$$ {z_t^j} \\text{ is the label predicted by oracle j at with expertise } \\phi_t $$\n",
    "$$ p({z_t^j}|\\phi{_t^j},y_t) = \\phi{_t^{j^{I(z{_t^j}=y_t)}}} (1-\\phi){_t^{j^{I(z{_t^j}=y_t)}}}$$\n",
    "\n",
    "y<sub>t</sub> is unknown. It's estimated from other labelers. Need better understanding to see what's happening.\n",
    "\n",
    "*A Probabilistic Framework to Learn from Multiple Annotators with Time-Varying Accuracy*\n",
    "*Pinar Donmez, Jaime Carbonell, Jeff Schneider *\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions \n",
    "\n",
    "* PAC is version space reduction?\n",
    "\n",
    "* Can we have synthesized queries?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Active Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meeting with Anurag\n",
    "\n",
    "* Soundnet might not give good representation. Suggested not to implement soundnet but rather, use existing models.\n",
    "* Most formulations are restricted to binary classification. Extend formulation to multiple classes.\n",
    "* Extending to evaluation of multiple classifiers is a good idea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meeting with Professor Saquib\n",
    "\n",
    "* Get a clearer idea of why we are doing what we are doing. Why and how it's important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meetings with Shaden\n",
    "\n",
    "** Convolutional Neural Network **\n",
    "\n",
    "Convolution Neural Network: Basic understanding of structure of Neural Network, purpose of Backpropagation. Backpropagation needs to be understood more thoroughly. Training procedure not clear yet. Questions:\n",
    "1. How to transfer filters from video to audio?\n",
    "2. Aren't we losing information about periodicity? \n",
    "3. How are varying length dealt with during training?\n",
    "4. If audio vectors are represented in a 2D matrix, what are the filters? There is no dependency between the rows, right?\n",
    "\n",
    "** HMM clustering **\n",
    "Rather than one HMM, there should be multiple local HMMs. Arrows within local HMMs learned using standard learning procedure. Problems: \n",
    "1. how to segment audio into local HMMs, \n",
    "2. what are the arrows between local HMMs? Uniform? learned from data -> (needs a lot of data)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
